{"cells":[{"cell_type":"code","execution_count":2,"id":"334d1ca5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of files: 60\n"]}],"source":["import sys\n","import pickle\n","from google.cloud import storage\n","\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","\n","BUCKET_NAME = \"ir_project_2025\"\n","\n","full_path = f\"gs://{BUCKET_NAME}/\"\n","paths=[]\n","\n","storage_client = storage.Client()\n","\n","blobs_read = client.list_blobs(BUCKET_NAME)\n","for b in blobs_read:\n","    if \"multistre\" in b.name:\n","        paths.append(full_path+b.name)\n","\n","print(\"Number of files: \" + str(len(paths)))"]},{"cell_type":"code","execution_count":3,"id":"0f5ff6c5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Safe to write. Destination 'id_to_title_dict/id_to_title.pkl' does not exist yet.\n","Reading Parquet files...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Extracting ID and Title pairs...\n","Collecting to Dictionary...\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["   Collected 6348910 pairs.\n","Saving to local pickle file...\n","Uploading...\n","\n","SUCCESS: Dictionary created and uploaded!\n","Location: gs://ir_project_2025/id_to_title_dict/id_to_title.pkl\n"]}],"source":["# Define the destination path \n","DESTINATION_FILENAME = \"id_to_title.pkl\"\n","FULL_DESTINATION_PATH = f\"id_to_title_dict/{DESTINATION_FILENAME}\"\n","\n","bucket = storage_client.bucket(BUCKET_NAME)\n","blob = bucket.blob(FULL_DESTINATION_PATH)\n","\n","# Safety Check: Prevent Overwriting\n","# This ensures we don't modify existing files or overwrite a previous run\n","if blob.exists():\n","    print(f\"SAFETY STOP: The file '{FULL_DESTINATION_PATH}' already exists in the bucket.\")\n","    print(\"To prevent accidental data loss, this code will not run. Please delete the file from the bucket or change the destination name.\")\n","else:\n","    print(f\"Safe to write. Destination '{FULL_DESTINATION_PATH}' does not exist yet.\")\n","\n","    try:\n","        print(\"Reading Parquet files...\")\n","        df_wiki = spark.read.parquet(*paths)\n","        \n","        print(\"Extracting ID and Title pairs...\")\n","        # Convert to RDD and map to (id, title) tuples\n","        # The .map() is required because collectAsMap expects tuples, not Rows\n","        id_title_pairs = df_wiki.select(\"id\", \"title\").rdd.map(lambda x: (x[0], x[1]))\n","        \n","        print(\"Collecting to Dictionary...\")\n","        id_to_title_dict = id_title_pairs.collectAsMap()\n","        \n","        print(f\"   Collected {len(id_to_title_dict)} pairs.\")\n","\n","        print(\"Saving to local pickle file...\")\n","        with open(DESTINATION_FILENAME, \"wb\") as f:\n","            pickle.dump(id_to_title_dict, f)\n","\n","        print(f\"Uploading...\")\n","        blob.upload_from_filename(DESTINATION_FILENAME)\n","        \n","        print(\"\\nSUCCESS: Dictionary created and uploaded!\")\n","        print(f\"Location: gs://{BUCKET_NAME}/{FULL_DESTINATION_PATH}\")\n","\n","    except Exception as e:\n","        print(f\"\\nError occurred: {str(e)}\")"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}